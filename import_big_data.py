import csv
import time
import numpy as np
from tqdm import tqdm # ‡∏´‡∏•‡∏≠‡∏î‡πÇ‡∏´‡∏•‡∏î

# Fix Numpy
if not hasattr(np, 'float_'): np.float_ = np.float64

from opensearchpy import OpenSearch, helpers
from sentence_transformers import SentenceTransformer

# Config
INDEX_NAME = "ecommerce_products"
BATCH_SIZE = 500  # ‡∏¢‡∏¥‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ DB ‡∏ó‡∏µ‡∏•‡∏∞ 500 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏µ)
CSV_FILE = "products_big.csv"

# Connect
client = OpenSearch(
    hosts=[{'host': 'localhost', 'port': 9200}],
    http_compress=True, use_ssl=False, verify_certs=False, timeout=60
)

# Model (‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
print("‚è≥ Loading AI Model (may take a moment)...")
model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2') 

def setup_index():
    print(f"üóëÔ∏è  Resetting Index: {INDEX_NAME}")
    if client.indices.exists(index=INDEX_NAME):
        client.indices.delete(index=INDEX_NAME)

    index_body = {
        "settings": {"index": {"knn": True}},
        "mappings": {
            "properties": {
                "title": {"type": "text"},
                "category": {"type": "keyword"},
                "price": {"type": "float"},
                "description": {"type": "text"},
                "vector_embedding": {
                    "type": "knn_vector",
                    "dimension": 768,
                    "method": {"name": "hnsw", "space_type": "cosinesimil", "engine": "nmslib"}
                }
            }
        }
    }
    client.indices.create(index=INDEX_NAME, body=index_body)
    print("‚úÖ Index Re-created!")

def import_big_data():
    if not client.ping():
        print("‚ùå Cannot connect to OpenSearch!")
        return

    setup_index()

    # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏´‡∏•‡∏≠‡∏î‡πÇ‡∏´‡∏•‡∏î
    print("üìä Counting rows...")
    with open(CSV_FILE, encoding='utf-8') as f:
        total_rows = sum(1 for line in f) - 1
    
    print(f"üöÄ Starting Import: {total_rows:,} items")
    print("‚òï Go grab a coffee, this will take a while...")

    actions = []
    
    with open(CSV_FILE, mode='r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        
        # ‡πÉ‡∏ä‡πâ tqdm ‡∏Ñ‡∏£‡∏≠‡∏ö reader ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏ä‡∏ß‡πå Progress Bar
        for row in tqdm(reader, total=total_rows, unit="item"):
            text = f"{row['title']} {row['description']} {row['category']}"
            vector = model.encode(text).tolist()
            
            doc = {
                "_index": INDEX_NAME,
                "_id": row['id'],
                "_source": {
                    "title": row['title'],
                    "description": row['description'],
                    "category": row['category'],
                    "price": float(row['price']),
                    "vector_embedding": vector
                }
            }
            actions.append(doc)
            
            # ‡∏ñ‡πâ‡∏≤‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ñ‡∏£‡∏ö Batch Size (500) ‡πÉ‡∏´‡πâ‡∏¢‡∏¥‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ DB ‡πÄ‡∏•‡∏¢
            if len(actions) >= BATCH_SIZE:
                helpers.bulk(client, actions)
                actions = [] # ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡πÅ‡∏£‡∏°

        # ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏Å‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
        if actions:
            helpers.bulk(client, actions)

    print("\nüéâ MISSION COMPLETE! 20,000 items imported.")

if __name__ == "__main__":
    import_big_data()